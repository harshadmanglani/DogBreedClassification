{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Input\n",
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = #refer to data preprocessing script to set this path\n",
    "test_dir =  #refer to data preprocessing script to set this path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "                rescale = 1./255, \n",
    "                shear_range = 0.2, \n",
    "                horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "validate_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper parameters\n",
    "batch_size = 32\n",
    "kernel_size = (2, 2) #wrong move\n",
    "filters = 64\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_datagen.flow_from_directory(train_dir, \n",
    "                                              batch_size = batch_size,\n",
    "                                              target_size = (100, 100),\n",
    "                                              color_mode = 'rgb',\n",
    "                                              class_mode = 'categorical')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(test_dir, \n",
    "                                            batch_size = batch_size,\n",
    "                                            target_size = (100, 100),\n",
    "                                            color_mode = 'rgb',\n",
    "                                            class_mode = 'categorical')\n",
    "\n",
    "test_set = validate_datagen.flow_from_directory(validate_dir, \n",
    "                                                batch_size = batch_size,\n",
    "                                                target_size = (100, 100),\n",
    "                                                color_mode = 'rgb',\n",
    "                                                class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model architecture\n",
    "def create_model(model):\n",
    "    #convolutional feedforwards\n",
    "    model.add(Conv2D(filters = filters, kernel_size = kernel_size, input_shape = (100, 100, 3), activation = 'relu'))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2)))\n",
    "    model.add(Conv2D(filters = filters, kernel_size = kernel_size, activation = 'relu'))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2)))\n",
    "    model.add(Conv2D(filters = filters, kernel_size = kernel_size, activation = 'sigmoid'))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2)))\n",
    "    model.add(Conv2D(filters = filters, kernel_size = kernel_size, activation = 'sigmoid'))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2)))\n",
    "\n",
    "    #flatten and connect it to the ANN\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units = 1200, activation = 'relu'))\n",
    "    model.add(Dense(units = 1000, activation = 'relu'))\n",
    "    model.add(Dense(units = 500, activation = 'relu'))\n",
    "    model.add(Dense(units = 400, activation = 'relu'))\n",
    "    model.add(Dense(units = 300, activation = 'relu'))\n",
    "    model.add(Dense(units = 120, activation = 'softmax'))\n",
    "    \n",
    "    model.compile(loss = 'categorical_crossentropy', \n",
    "              optimizer = 'adam',\n",
    "             metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  Sequential()\n",
    "model = create_model(model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#added features for accurate and optimized training\n",
    "# a check point callback to save our best weights\n",
    "filepath = #set path\n",
    "checkpoint = ModelCheckpoint(filepath, \n",
    "                             monitor='val_accuracy', \n",
    "                             verbose=1, \n",
    "                             save_best_only=False, \n",
    "                             mode='max', \n",
    "                             save_weights_only=False)\n",
    "\n",
    "# a reducing lr callback to reduce lr when val_loss doesn't increase\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                                   patience=1, verbose=1, mode='min',\n",
    "                                   min_delta=0.0001, cooldown=2, min_lr=1e-7)\n",
    "\n",
    "# for early stop\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_set, \n",
    "          epochs = epochs, \n",
    "          validation_data = test_set,\n",
    "          validation_steps = 479,\n",
    "          verbose = 1, \n",
    "          steps_per_epoch = 2009, \n",
    "          callbacks=[checkpoint, reduce_lr, early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_final_history(history):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "    ax[0].set_title('loss')\n",
    "    ax[0].plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n",
    "    ax[0].plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\n",
    "    ax[1].set_title('acc')\n",
    "    ax[1].plot(history.epoch, history.history[\"accuracy\"], label=\"Train acc\")\n",
    "    ax[1].plot(history.epoch, history.history[\"val_accuracy\"], label=\"Validation acc\")\n",
    "    ax[0].legend()\n",
    "    ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_final_history(history)\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('beagle.jpg')\n",
    "res = cv2.resize(img, dsize=(100, 100))\n",
    "res = res/255.0\n",
    "res = np.asarray(res)\n",
    "res = res.reshape((-1, 100, 100, 3))\n",
    "y_prob = model.predict(res) \n",
    "index = np.argmax(y_prob)\n",
    "labels = {value: key for key, value in train_set.class_indices.items()}\n",
    "print(labels[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_score = model.evaluate(test_set)\n",
    "print(\"Model Test Loss:\",model_score[0])\n",
    "print(\"Model Test Accuracy:\",model_score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
